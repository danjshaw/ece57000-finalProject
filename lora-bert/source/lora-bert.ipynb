{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danjshaw/ece57000-finalProject/blob/main/lora-bert/source/lora-bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "RJc8lRi2sPF3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OOOR51p7nJQb",
        "outputId": "57370617-7bdb-4634-f676-c40277dc6bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_rJOUWbHb7h7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiAkOY-564eO",
        "outputId": "94991390-e48c-4138-9176-4c003989d7e1",
        "collapsed": true
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "VDqSTwjJilGA",
        "collapsed": true
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "dnum5mxHf-MI"
      },
      "outputs": [],
      "source": [
        "def get_trainable_parameters(model):\n",
        "  trainable_parameters = 0\n",
        "  parameters = 0\n",
        "  for param in model.parameters():\n",
        "    parameters += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_parameters += param.numel()\n",
        "  return {'total_parameters': parameters, 'trainable_parameters': trainable_parameters}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "def write_results_to_csv(file_name, results):\n",
        "  with open(file_name, 'w', newline='') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=list(results[0].keys()))\n",
        "      writer.writeheader()\n",
        "      for result in results:\n",
        "        writer.writerow(result)"
      ],
      "metadata": {
        "id": "yexmh39lLLnX"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_name(name_and_path):\n",
        "  return name_and_path.split('/')[1]"
      ],
      "metadata": {
        "id": "zLuF2DieXmzO"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Fine-Tuning Implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "6pdjddZVsbQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Followed examples from this [Hugging Face NLP course](https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt#fine-tuning-a-model-with-the-trainer-api) on how to use the trainer API for fine-tuning."
      ],
      "metadata": {
        "id": "xM1jwv_maqfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import time\n",
        "\n",
        "def ft_hyperparameter_tune(checkpoint, epochs, batch_sizes, learning_rates):\n",
        "  model_name = checkpoint\n",
        "  if '/' in checkpoint:\n",
        "    model_name = get_model_name(checkpoint)\n",
        "\n",
        "  raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  def tokenize_function(example):\n",
        "      return tokenizer(example[\"sentence1\"], example[\"sentence2\"])\n",
        "\n",
        "  tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "  def compute_metrics(eval_preds):\n",
        "      metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "      logits, labels = eval_preds\n",
        "      predictions = np.argmax(logits, axis=-1)\n",
        "      return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "  ft_results = []\n",
        "  progress = 0\n",
        "  iterations = len(batch_sizes)*len(learning_rates)\n",
        "\n",
        "  # Hyperparameter tuning\n",
        "  start_time = time.time()\n",
        "  for size in batch_sizes:\n",
        "    for rate in learning_rates:\n",
        "      # Track progress\n",
        "      progress += 1\n",
        "      print(f\"Progress: {progress}/{iterations}\")\n",
        "\n",
        "      # Initialize result\n",
        "      result = {\"batch_size\": size, \"learning_rate\": rate}\n",
        "\n",
        "      model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)\n",
        "\n",
        "      # Setup the trainer\n",
        "      training_args = TrainingArguments(\n",
        "          \"fine-tuning-trainer\",\n",
        "          eval_strategy=\"epoch\",\n",
        "          per_device_eval_batch_size=size,\n",
        "          per_device_train_batch_size=size,\n",
        "          num_train_epochs=epochs,\n",
        "          learning_rate=rate,\n",
        "          disable_tqdm=True,\n",
        "          report_to=\"none\"\n",
        "      )\n",
        "      trainer = Trainer(\n",
        "          model,\n",
        "          training_args,\n",
        "          train_dataset=tokenized_datasets[\"train\"],\n",
        "          eval_dataset=tokenized_datasets[\"validation\"],\n",
        "          data_collator=data_collator,\n",
        "          processing_class=tokenizer,\n",
        "          compute_metrics=compute_metrics\n",
        "      )\n",
        "\n",
        "      # Train the model\n",
        "      ft_results.append(result | trainer.train().metrics | trainer.evaluate() | get_trainable_parameters(model))\n",
        "\n",
        "  write_results_to_csv(output_dir+model_name+'-ft-results.csv', ft_results)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  runtime_seconds = end_time - start_time\n",
        "  runtime_minutes = runtime_seconds / 60\n",
        "\n",
        "  # Output the best result\n",
        "  max_ft_result = ft_results[0]\n",
        "  for _, result in enumerate(ft_results):\n",
        "    if result['eval_f1'] > max_ft_result['eval_f1']:\n",
        "      max_ft_result = result\n",
        "  print(f'\\n================ \\\n",
        "          \\nTotal Runtime: {runtime_minutes} minutes \\\n",
        "          \\nBest Result: \\\n",
        "          \\n\\tF1={max_ft_result[\"eval_f1\"]} \\\n",
        "          \\n\\tBatch Size={max_ft_result[\"batch_size\"]} \\\n",
        "          \\n\\tLearning Rate={max_ft_result[\"learning_rate\"]}'\n",
        "  )\n",
        "\n",
        "  return ft_results"
      ],
      "metadata": {
        "id": "R0rtaL56jMN3",
        "collapsed": true
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "def fine_tune(checkpoint, epoch, batch_size, learning_rate):\n",
        "  model_name = checkpoint\n",
        "  if '/' in checkpoint:\n",
        "    model_name = get_model_name(checkpoint)\n",
        "\n",
        "  raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  def tokenize_function(example):\n",
        "      return tokenizer(example[\"sentence1\"], example[\"sentence2\"])\n",
        "\n",
        "  tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "  def compute_metrics(eval_preds):\n",
        "      metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "      logits, labels = eval_preds\n",
        "      predictions = np.argmax(logits, axis=-1)\n",
        "      return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)\n",
        "\n",
        "  # Setup the trainer\n",
        "  training_args = TrainingArguments(\n",
        "      \"fine-tuning-trainer\",\n",
        "      eval_strategy=\"epoch\",\n",
        "      per_device_eval_batch_size=batch_size,\n",
        "      per_device_train_batch_size=batch_size,\n",
        "      num_train_epochs=epoch,\n",
        "      learning_rate=learning_rate,\n",
        "      report_to=\"none\"\n",
        "  )\n",
        "  trainer = Trainer(\n",
        "      model,\n",
        "      training_args,\n",
        "      train_dataset=tokenized_datasets[\"train\"],\n",
        "      eval_dataset=tokenized_datasets[\"validation\"],\n",
        "      data_collator=data_collator,\n",
        "      processing_class=tokenizer,\n",
        "      compute_metrics=compute_metrics\n",
        "  )\n",
        "\n",
        "  # Train the model\n",
        "  result = trainer.train().metrics | trainer.evaluate() | get_trainable_parameters(model)\n",
        "\n",
        "  print(\"=====================\")\n",
        "  print(\"Results:\")\n",
        "  for key in result:\n",
        "    print(f'\\t{key}={result[key]}')"
      ],
      "metadata": {
        "id": "DDspdtXlJ0-K"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Rank Adaptation (LoRA) Implementation"
      ],
      "metadata": {
        "id": "fn2hce2EzI11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "MivaCxsh7UKy"
      },
      "outputs": [],
      "source": [
        "class LoraModule(nn.Module):\n",
        "  def __init__(self, in_features, out_features, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.scale = alpha / rank\n",
        "    self.A = nn.Parameter(torch.randn(in_features, rank))\n",
        "    self.B = nn.Parameter(torch.zeros(rank, out_features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return (self.scale * (x @ self.A @ self.B))\n",
        "\n",
        "class LoraLinear(nn.Module):\n",
        "  def __init__(self, linear, rank, alpha):\n",
        "    super().__init__()\n",
        "    if (isinstance(linear, LoraLinear)):\n",
        "      self.linear = linear.linear\n",
        "      self.lora = LoraModule(self.linear.in_features, self.linear.out_features, rank, alpha)\n",
        "\n",
        "    else:\n",
        "      self.linear = linear\n",
        "      self.lora = LoraModule(self.linear.in_features, self.linear.out_features, rank, alpha)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x) + self.lora(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_lora_model(model, rank, alpha):\n",
        "  # Replace the query and value linear layers with LoRA layers\n",
        "  for _, layer in enumerate(model.bert.encoder.layer):\n",
        "    s = layer.attention.self\n",
        "    s.query = LoraLinear(s.query, rank, alpha)\n",
        "    s.value = LoraLinear(s.value, rank, alpha)\n",
        "\n",
        "  # Freeze the pre-trained weights\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'A' in name or 'B' in name:\n",
        "      param.requires_grad = True\n",
        "    else:\n",
        "      param.requires_grad = False"
      ],
      "metadata": {
        "id": "G32Km33dIKU5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import time\n",
        "\n",
        "def lora_hyperparameter_tune(checkpoint, ranks, alphas, epochs, batch_sizes, learning_rates):\n",
        "  model_name = checkpoint\n",
        "  if '/' in checkpoint:\n",
        "    model_name = get_model_name(checkpoint)\n",
        "\n",
        "  # Setup model and dataset\n",
        "  raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  def tokenize_function(example):\n",
        "      return tokenizer(example[\"sentence1\"], example[\"sentence2\"])\n",
        "\n",
        "  tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "  def compute_metrics(eval_preds):\n",
        "      metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "      logits, labels = eval_preds\n",
        "      predictions = np.argmax(logits, axis=-1)\n",
        "      return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)\n",
        "\n",
        "  lora_results = []\n",
        "  progress = 0\n",
        "  iterations = len(batch_sizes)*len(learning_rates)*len(ranks)*len(alphas)\n",
        "\n",
        "  # Hyperparameter tuning\n",
        "  start_time = time.time()\n",
        "  for rank in ranks:\n",
        "    for alpha in alphas:\n",
        "      for size in batch_sizes:\n",
        "        for rate in learning_rates:\n",
        "          # Track progress\n",
        "          progress += 1\n",
        "          print(f\"Progress: {progress}/{iterations}\")\n",
        "\n",
        "          # Initialize result\n",
        "          result = {\"rank\": rank, \"alpha\": alpha, \"batch_size\": size, \"learning_rate\": rate}\n",
        "\n",
        "          # Configure an existing model with new LoRA layers\n",
        "          configure_lora_model(model, rank, alpha)\n",
        "\n",
        "          # Setup the trainer\n",
        "          training_args = TrainingArguments(\n",
        "              \"lora-trainer\",\n",
        "              eval_strategy=\"epoch\",\n",
        "              per_device_eval_batch_size=size,\n",
        "              per_device_train_batch_size=size,\n",
        "              num_train_epochs=epochs,\n",
        "              learning_rate=rate,\n",
        "              disable_tqdm=True,\n",
        "              report_to=\"none\"\n",
        "          )\n",
        "          trainer = Trainer(\n",
        "              model,\n",
        "              training_args,\n",
        "              train_dataset=tokenized_datasets[\"train\"],\n",
        "              eval_dataset=tokenized_datasets[\"validation\"],\n",
        "              data_collator=data_collator,\n",
        "              processing_class=tokenizer,\n",
        "              compute_metrics=compute_metrics\n",
        "          )\n",
        "\n",
        "          # Train the model\n",
        "          lora_results.append(result | trainer.train().metrics | trainer.evaluate() | get_trainable_parameters(model))\n",
        "\n",
        "  write_results_to_csv(output_dir+model_name+'-lora-results.csv', lora_results)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  runtime_seconds = end_time - start_time\n",
        "  runtime_hours = runtime_seconds / 3600\n",
        "\n",
        "  # Output the best result\n",
        "  max_lora_result = lora_results[0]\n",
        "  for _, result in enumerate(lora_results):\n",
        "    if result['eval_f1'] > max_lora_result['eval_f1']:\n",
        "      max_lora_result = result\n",
        "  print(f'\\n================ \\\n",
        "          \\nTotal Runtime: {runtime_hours} hours \\\n",
        "          \\nBest Result: \\\n",
        "          \\n\\tF1={max_lora_result[\"eval_f1\"]} \\\n",
        "          \\n\\tRank={max_lora_result[\"rank\"]} \\\n",
        "          \\n\\tAlpha={max_lora_result[\"alpha\"]} \\\n",
        "          \\n\\tBatch Size={max_lora_result[\"batch_size\"]} \\\n",
        "          \\n\\tLearning Rate={max_lora_result[\"learning_rate\"]}'\n",
        "  )\n",
        "\n",
        "  return lora_results"
      ],
      "metadata": {
        "id": "2sRCUocTD3eo",
        "collapsed": true
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "def fine_tune_with_lora(checkpoint, rank, alpha, epoch, batch_size, learning_rate):\n",
        "  model_name = checkpoint\n",
        "  if '/' in checkpoint:\n",
        "    model_name = get_model_name(checkpoint)\n",
        "\n",
        "  # Setup model and dataset\n",
        "  raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  def tokenize_function(example):\n",
        "      return tokenizer(example[\"sentence1\"], example[\"sentence2\"])\n",
        "\n",
        "  tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "  def compute_metrics(eval_preds):\n",
        "      metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "      logits, labels = eval_preds\n",
        "      predictions = np.argmax(logits, axis=-1)\n",
        "      return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)\n",
        "\n",
        "  # Configure an existing model with new LoRA layers\n",
        "  configure_lora_model(model, rank, alpha)\n",
        "\n",
        "  # Setup the trainer\n",
        "  training_args = TrainingArguments(\n",
        "      \"lora-trainer\",\n",
        "      eval_strategy=\"epoch\",\n",
        "      per_device_eval_batch_size=batch_size,\n",
        "      per_device_train_batch_size=batch_size,\n",
        "      num_train_epochs=epoch,\n",
        "      learning_rate=learning_rate,\n",
        "      report_to=\"none\"\n",
        "  )\n",
        "  trainer = Trainer(\n",
        "      model,\n",
        "      training_args,\n",
        "      train_dataset=tokenized_datasets[\"train\"],\n",
        "      eval_dataset=tokenized_datasets[\"validation\"],\n",
        "      data_collator=data_collator,\n",
        "      processing_class=tokenizer,\n",
        "      compute_metrics=compute_metrics\n",
        "  )\n",
        "\n",
        "  # Train the model\n",
        "  result = trainer.train().metrics | trainer.evaluate() | get_trainable_parameters(model)\n",
        "\n",
        "  print(\"=====================\")\n",
        "  print(\"Results:\")\n",
        "  for key in result:\n",
        "    print(f'\\t{key}={result[key]}')"
      ],
      "metadata": {
        "id": "D-D2LoKyJA0D"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "5BssM-XUfY3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"prajjwal1/bert-tiny\", \"prajjwal1/bert-mini\", \"prajjwal1/bert-small\"]"
      ],
      "metadata": {
        "id": "1mOBwWn2HCsu"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "xesfWY1QG6zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters from [google-research/bert](https://github.com/google-research/bert):\n",
        "\n",
        "\n",
        "\n",
        "> For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs:\n",
        "> * batch sizes: 8, 16, 32, 64, 128\n",
        "> * learning rates: 3e-4, 1e-4, 5e-5, 3e-5\n",
        "\n"
      ],
      "metadata": {
        "id": "aZnBuTehsapX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4\n",
        "batch_sizes = [8, 16, 32, 64, 128]\n",
        "learning_rates = [3e-4, 1e-4, 5e-5, 3e-5]"
      ],
      "metadata": {
        "id": "62XZoxhusMhO"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranks = [1, 2, 4, 8, 16]\n",
        "alphas = [1, 2, 4, 8, 16]"
      ],
      "metadata": {
        "id": "niceYTIvGnz2"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment to perform hyperparameter tuningâ€”this is a long process and it might be beneficial to break it up over multiple cells if desired. Results metrics are saved to CSV files in google drive for further analysis and interpretation."
      ],
      "metadata": {
        "id": "49T24MHhHaZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s1zuELS-Znol"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# if not os.path.exists('/content/drive/MyDrive/lora-bert/'):\n",
        "#     os.makedirs('/content/drive/MyDrive/lora-bert/')\n",
        "# output_dir = '/content/drive/MyDrive/lora-bert/'"
      ],
      "metadata": {
        "id": "rmH2TqY1qMYw"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for model in models:\n",
        "#   _ = ft_hyperparameter_tune(model, epochs, batch_sizes, learning_rates)\n",
        "#   _ = lora_hyperparameter_tune(model, ranks, alphas, epochs, batch_sizes, learning_rates)"
      ],
      "metadata": {
        "id": "EDDl1dUAHPU-"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Hyperparameters"
      ],
      "metadata": {
        "id": "8iWMjhuEIGqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune(\"prajjwal1/bert-tiny\", epoch=4, batch_size=64, learning_rate=3e-4)"
      ],
      "metadata": {
        "id": "1hR5i7UCIRXs",
        "outputId": "e75fbf69-8ecc-4b8e-8162-764ca83937f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='232' max='232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [232/232 00:10, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.578104</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.820359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.557935</td>\n",
              "      <td>0.737745</td>\n",
              "      <td>0.834621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.565318</td>\n",
              "      <td>0.754902</td>\n",
              "      <td>0.841270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.580859</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.837061</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================\n",
            "Results:\n",
            "\ttrain_runtime=11.0653\n",
            "\ttrain_samples_per_second=1325.942\n",
            "\ttrain_steps_per_second=20.966\n",
            "\ttotal_flos=3035719599600.0\n",
            "\ttrain_loss=0.4791317643790409\n",
            "\tepoch=4.0\n",
            "\teval_loss=0.5808593034744263\n",
            "\teval_accuracy=0.75\n",
            "\teval_f1=0.8370607028753994\n",
            "\teval_runtime=1.2949\n",
            "\teval_samples_per_second=315.078\n",
            "\teval_steps_per_second=5.406\n",
            "\ttotal_parameters=4386178\n",
            "\ttrainable_parameters=4386178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_with_lora(\"prajjwal1/bert-tiny\", rank=4, alpha=8, epoch=4, batch_size=8, learning_rate=3e-4)"
      ],
      "metadata": {
        "id": "vwCslIBfIuvL",
        "outputId": "3af95aa0-e686-4ece-d823-9f3a55d60865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1836' max='1836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1836/1836 00:24, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.614409</td>\n",
              "      <td>0.683824</td>\n",
              "      <td>0.812227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.634000</td>\n",
              "      <td>0.606059</td>\n",
              "      <td>0.686275</td>\n",
              "      <td>0.813411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.619700</td>\n",
              "      <td>0.602576</td>\n",
              "      <td>0.693627</td>\n",
              "      <td>0.816984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.606500</td>\n",
              "      <td>0.602209</td>\n",
              "      <td>0.698529</td>\n",
              "      <td>0.818316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================\n",
            "Results:\n",
            "\ttrain_runtime=24.5677\n",
            "\ttrain_samples_per_second=597.206\n",
            "\ttrain_steps_per_second=74.732\n",
            "\ttotal_flos=2635525921248.0\n",
            "\ttrain_loss=0.6190774788783786\n",
            "\tepoch=4.0\n",
            "\teval_loss=0.6022091507911682\n",
            "\teval_accuracy=0.6985294117647058\n",
            "\teval_f1=0.8183161004431314\n",
            "\teval_runtime=1.5603\n",
            "\teval_samples_per_second=261.483\n",
            "\teval_steps_per_second=32.685\n",
            "\ttotal_parameters=4390274\n",
            "\ttrainable_parameters=4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune(\"prajjwal1/bert-mini\", epoch=4, batch_size=16, learning_rate=5e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "r3geWDzjK0kY",
        "outputId": "6a7c16ce-72ba-44bc-dbcf-b013465effc7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [920/920 00:24, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.499999</td>\n",
              "      <td>0.757353</td>\n",
              "      <td>0.835275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.458275</td>\n",
              "      <td>0.781863</td>\n",
              "      <td>0.852405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.516400</td>\n",
              "      <td>0.449616</td>\n",
              "      <td>0.796569</td>\n",
              "      <td>0.851521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.516400</td>\n",
              "      <td>0.461786</td>\n",
              "      <td>0.806373</td>\n",
              "      <td>0.862609</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [26/26 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================\n",
            "Results:\n",
            "\ttrain_runtime=24.8562\n",
            "\ttrain_samples_per_second=590.275\n",
            "\ttrain_steps_per_second=37.013\n",
            "\ttotal_flos=21529427905728.0\n",
            "\ttrain_loss=0.43540860051694125\n",
            "\tepoch=4.0\n",
            "\teval_loss=0.46178555488586426\n",
            "\teval_accuracy=0.8063725490196079\n",
            "\teval_f1=0.8626086956521739\n",
            "\teval_runtime=1.5939\n",
            "\teval_samples_per_second=255.976\n",
            "\teval_steps_per_second=16.312\n",
            "\ttotal_parameters=11171074\n",
            "\ttrainable_parameters=11171074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_with_lora(\"prajjwal1/bert-mini\", rank=16, alpha=4, epoch=4, batch_size=8, learning_rate=3e-4)"
      ],
      "metadata": {
        "id": "iFJtjhTzK6aQ",
        "outputId": "e3496644-2bbe-4d6a-f10e-444e32adfe1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1836' max='1836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1836/1836 00:33, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.544532</td>\n",
              "      <td>0.740196</td>\n",
              "      <td>0.832278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.596100</td>\n",
              "      <td>0.516101</td>\n",
              "      <td>0.762255</td>\n",
              "      <td>0.840198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.547800</td>\n",
              "      <td>0.502720</td>\n",
              "      <td>0.772059</td>\n",
              "      <td>0.843170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.511800</td>\n",
              "      <td>0.513373</td>\n",
              "      <td>0.762255</td>\n",
              "      <td>0.840722</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================\n",
            "Results:\n",
            "\ttrain_runtime=33.75\n",
            "\ttrain_samples_per_second=434.725\n",
            "\ttrain_steps_per_second=54.4\n",
            "\ttotal_flos=20769117438432.0\n",
            "\ttrain_loss=0.5442345978647536\n",
            "\tepoch=4.0\n",
            "\teval_loss=0.5133726000785828\n",
            "\teval_accuracy=0.7622549019607843\n",
            "\teval_f1=0.8407224958949097\n",
            "\teval_runtime=1.5326\n",
            "\teval_samples_per_second=266.21\n",
            "\teval_steps_per_second=33.276\n",
            "\ttotal_parameters=11236610\n",
            "\ttrainable_parameters=65536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune(\"prajjwal1/bert-small\", epoch=4, batch_size=8, learning_rate=3e-5)"
      ],
      "metadata": {
        "id": "4p8BHNuQLCSw",
        "outputId": "0246aa40-0f88-41c2-ca4e-9ce6c86cacee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1836' max='1836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1836/1836 01:00, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.521150</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.839506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.572900</td>\n",
              "      <td>0.405591</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.878788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.393100</td>\n",
              "      <td>0.419380</td>\n",
              "      <td>0.838235</td>\n",
              "      <td>0.882979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.251600</td>\n",
              "      <td>0.597334</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.885906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================\n",
            "Results:\n",
            "\ttrain_runtime=60.5071\n",
            "\ttrain_samples_per_second=242.484\n",
            "\ttrain_steps_per_second=30.344\n",
            "\ttotal_flos=81238114922976.0\n",
            "\ttrain_loss=0.3667250365213631\n",
            "\tepoch=4.0\n",
            "\teval_loss=0.5973342061042786\n",
            "\teval_accuracy=0.8333333333333334\n",
            "\teval_f1=0.8859060402684564\n",
            "\teval_runtime=1.5152\n",
            "\teval_samples_per_second=269.274\n",
            "\teval_steps_per_second=33.659\n",
            "\ttotal_parameters=28764674\n",
            "\ttrainable_parameters=28764674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_with_lora(\"prajjwal1/bert-small\", rank=16, alpha=16, epoch=4, batch_size=8, learning_rate=1e-4)"
      ],
      "metadata": {
        "id": "2lqe1P5DLLPn",
        "outputId": "3f65ae75-d6f4-4498-b700-cdf218c2047c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1836' max='1836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1836/1836 00:37, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.548451</td>\n",
              "      <td>0.715686</td>\n",
              "      <td>0.823708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.604100</td>\n",
              "      <td>0.509534</td>\n",
              "      <td>0.747549</td>\n",
              "      <td>0.831974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.544500</td>\n",
              "      <td>0.493069</td>\n",
              "      <td>0.762255</td>\n",
              "      <td>0.832470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.490400</td>\n",
              "      <td>0.498056</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.840532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================\n",
            "Results:\n",
            "\ttrain_runtime=37.3606\n",
            "\ttrain_samples_per_second=392.713\n",
            "\ttrain_steps_per_second=49.143\n",
            "\ttotal_flos=82065196020192.0\n",
            "\ttrain_loss=0.5331009343298951\n",
            "\tepoch=4.0\n",
            "\teval_loss=0.49805593490600586\n",
            "\teval_accuracy=0.7647058823529411\n",
            "\teval_f1=0.840531561461794\n",
            "\teval_runtime=1.589\n",
            "\teval_samples_per_second=256.76\n",
            "\teval_steps_per_second=32.095\n",
            "\ttotal_parameters=28895746\n",
            "\ttrainable_parameters=131072\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMlRWiocyyJFzvl1kQRM72l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}