{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danjshaw/ece57000-finalProject/blob/main/lora-bert/source/lora-bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "RJc8lRi2sPF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s1zuELS-Znol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b2acea-3a3a-44fc-e824-d6d69717fcfd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists('/content/drive/MyDrive/lora-bert/'):\n",
        "    os.makedirs('/content/drive/MyDrive/lora-bert/')\n",
        "output_dir = '/content/drive/MyDrive/lora-bert/'"
      ],
      "metadata": {
        "id": "rmH2TqY1qMYw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OOOR51p7nJQb",
        "outputId": "af938c1f-c50e-48da-fd5a-303d72e72a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: codecarbon in /usr/local/lib/python3.10/dist-packages (2.7.4)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.10/dist-packages (from codecarbon) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon) (8.1.7)\n",
            "Requirement already satisfied: fief-client[cli] in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from codecarbon) (11.5.3)\n",
            "Requirement already satisfied: questionary in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.0.1)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from codecarbon) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.9.0.20241003)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
            "Requirement already satisfied: yaspin in /usr/local/lib/python3.10/dist-packages (from fief-client[cli]->codecarbon) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\n",
            "Requirement already satisfied: prompt_toolkit<=3.0.36,>=2.0 in /usr/local/lib/python3.10/dist-packages (from questionary->codecarbon) (3.0.36)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.10/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<=3.0.36,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
            "Requirement already satisfied: termcolor<2.4.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from yaspin->fief-client[cli]->codecarbon) (2.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install codecarbon\n",
        "!pip freeze > /content/drive/MyDrive/lora-bert-tiny/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from codecarbon import EmissionsTracker"
      ],
      "metadata": {
        "id": "_rJOUWbHb7h7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiAkOY-564eO",
        "outputId": "82bbf073-c5e2-4684-f478-fbcffe7bd178"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "VDqSTwjJilGA",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dnum5mxHf-MI"
      },
      "outputs": [],
      "source": [
        "def get_trainable_parameters(model):\n",
        "  trainable_parameters = 0\n",
        "  parameters = 0\n",
        "  for param in model.parameters():\n",
        "    parameters += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_parameters += param.numel()\n",
        "  return {'total_parameters': parameters, 'trainable_parameters': trainable_parameters}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "def write_results_to_csv(file_name, results):\n",
        "  with open(file_name, 'w', newline='') as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=list(results[0].keys()))\n",
        "      writer.writeheader()\n",
        "      for result in results:\n",
        "        writer.writerow(result)"
      ],
      "metadata": {
        "id": "yexmh39lLLnX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_name(name_and_path):\n",
        "  return name_and_path.split('/')[1]"
      ],
      "metadata": {
        "id": "zLuF2DieXmzO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Fine-Tuning\n",
        "\n"
      ],
      "metadata": {
        "id": "6pdjddZVsbQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Followed examples from this [Hugging Face NLP course](https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt#fine-tuning-a-model-with-the-trainer-api) on how to use the trainer API for fine-tuning."
      ],
      "metadata": {
        "id": "xM1jwv_maqfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "def fine_tune(checkpoint, epochs, batch_size, learning_rate):\n",
        "  model_name = checkpoint\n",
        "  if '/' in checkpoint:\n",
        "    model_name = get_model_name(checkpoint)\n",
        "\n",
        "  # Initialize result\n",
        "  result = {\"batch_size\": batch_size, \"learning_rate\": learning_rate}\n",
        "\n",
        "  # Track emissions\n",
        "  tracker = EmissionsTracker('fine-tuning-' + model_name, save_to_file=False)\n",
        "  try:\n",
        "    tracker.start()\n",
        "\n",
        "    # Setup model and dataset\n",
        "    raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "    def tokenize_function(example):\n",
        "        return tokenizer(example[\"sentence1\"], example[\"sentence2\"])\n",
        "\n",
        "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    def compute_metrics(eval_preds):\n",
        "        metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "        logits, labels = eval_preds\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)\n",
        "\n",
        "    # Setup the trainer\n",
        "    training_args = TrainingArguments(\n",
        "        \"fine-tuning-trainer\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        learning_rate=batch_size,\n",
        "        disable_tqdm=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "        processing_class=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    result |= trainer.train().metrics\n",
        "\n",
        "    # Stop tracking emissions\n",
        "    tracker.stop()\n",
        "\n",
        "    # Store metrics and delete the tracker\n",
        "    result |= trainer.evaluate() | get_trainable_parameters(model) | tracker.final_emissions_data.values\n",
        "  finally:\n",
        "    del tracker\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "AQt5byIxFRrJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, set_seed\n",
        "import time\n",
        "\n",
        "def hyperparameter_tune(checkpoint, batch_sizes, learning_rates):\n",
        "  model_name = checkpoint\n",
        "  if '/' in checkpoint:\n",
        "    model_name = get_model_name(checkpoint)\n",
        "\n",
        "  ft_results = []\n",
        "  progress = 0\n",
        "  iterations = len(batch_sizes)*len(learning_rates)\n",
        "\n",
        "  # Hyperparameter tuning\n",
        "  start_time = time.time()\n",
        "  for size in batch_sizes:\n",
        "    for rate in learning_rates:\n",
        "      # Track progress\n",
        "      progress += 1\n",
        "      print(f\"Progress: {progress}/{iterations}\")\n",
        "\n",
        "      # Fine-tune the model and store the results\n",
        "      ft_result = fine_tune(checkpoint, epochs, size, rate)\n",
        "      ft_results.append(ft_result)\n",
        "\n",
        "  write_results_to_csv(output_dir + model_name + '-ft-results.csv', ft_results)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  runtime_seconds = end_time - start_time\n",
        "  runtime_minutes = runtime_seconds / 60\n",
        "\n",
        "  # Output the best result\n",
        "  max_ft_result = ft_results[0]\n",
        "  for _, result in enumerate(ft_results):\n",
        "    if result['eval_f1'] > max_ft_result['eval_f1']:\n",
        "      max_ft_result = result\n",
        "  print(f'\\n================ \\\n",
        "          \\nTotal Runtime: {runtime_minutes} minutes \\\n",
        "          \\nBest Result: \\\n",
        "          \\n\\tF1={max_ft_result[\"eval_f1\"]} \\\n",
        "          \\n\\tBatch Size={max_ft_result[\"batch_size\"]} \\\n",
        "          \\n\\tLearning Rate={max_ft_result[\"learning_rate\"]}'\n",
        "  )\n",
        "\n",
        "  return ft_results"
      ],
      "metadata": {
        "id": "R0rtaL56jMN3",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Rank Adaptation (LoRA)"
      ],
      "metadata": {
        "id": "fn2hce2EzI11"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MivaCxsh7UKy"
      },
      "outputs": [],
      "source": [
        "class LoraModule(nn.Module):\n",
        "  def __init__(self, in_features, out_features, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.scale = alpha / rank\n",
        "    self.A = nn.Parameter(torch.randn(in_features, rank))\n",
        "    self.B = nn.Parameter(torch.zeros(rank, out_features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return (self.scale * (x @ self.A @ self.B))\n",
        "\n",
        "class LoraLinear(nn.Module):\n",
        "  def __init__(self, linear, rank, alpha):\n",
        "    super().__init__()\n",
        "    if (isinstance(linear, LoraLinear)):\n",
        "      self.linear = linear.linear\n",
        "      self.lora = LoraModule(self.linear.in_features, self.linear.out_features, rank, alpha)\n",
        "\n",
        "    else:\n",
        "      self.linear = linear\n",
        "      self.lora = LoraModule(self.linear.in_features, self.linear.out_features, rank, alpha)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x) + self.lora(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_lora_model(model, rank, alpha):\n",
        "  # Replace the query and value linear layers with LoRA layers\n",
        "  for _, layer in enumerate(model.bert.encoder.layer):\n",
        "    s = layer.attention.self\n",
        "    s.query = LoraLinear(s.query, rank, alpha)\n",
        "    s.value = LoraLinear(s.value, rank, alpha)\n",
        "\n",
        "  # Freeze the pre-trained weights\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'A' in name or 'B' in name:\n",
        "      param.requires_grad = True\n",
        "    else:\n",
        "      param.requires_grad = False"
      ],
      "metadata": {
        "id": "G32Km33dIKU5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, set_seed\n",
        "import time\n",
        "\n",
        "def lora_hyperparameter_tune(checkpoint, ranks, alphas, epochs, batch_sizes, learning_rates):\n",
        "  model_name = checkpoint\n",
        "  if '/' in checkpoint:\n",
        "    model_name = get_model_name(checkpoint)\n",
        "\n",
        "  # Setup model and dataset\n",
        "  raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  def tokenize_function(example):\n",
        "      return tokenizer(example[\"sentence1\"], example[\"sentence2\"])\n",
        "\n",
        "  tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "  def compute_metrics(eval_preds):\n",
        "      metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "      logits, labels = eval_preds\n",
        "      predictions = np.argmax(logits, axis=-1)\n",
        "      return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)\n",
        "\n",
        "  lora_results = []\n",
        "  progress = 0\n",
        "  iterations = len(batch_sizes)*len(learning_rates)*len(ranks)*len(alphas)\n",
        "\n",
        "  # Hyperparameter tuning\n",
        "  start_time = time.time()\n",
        "  for rank in ranks:\n",
        "    for alpha in alphas:\n",
        "      for size in batch_sizes:\n",
        "        for rate in learning_rates:\n",
        "          # Track progress\n",
        "          progress += 1\n",
        "          print(f\"Progress: {progress}/{iterations}\")\n",
        "\n",
        "          # Initialize result\n",
        "          result = {\"rank\": rank, \"alpha\": alpha, \"batch_size\": size, \"learning_rate\": rate}\n",
        "\n",
        "          # Track emissions\n",
        "          tracker = EmissionsTracker('lora-bert-tiny', save_to_file=False)\n",
        "          try:\n",
        "            tracker.start()\n",
        "\n",
        "            # Configure an existing model with new LoRA layers\n",
        "            configure_lora_model(model, rank, alpha)\n",
        "\n",
        "            # Setup the trainer\n",
        "            training_args = TrainingArguments(\n",
        "                \"lora-trainer\",\n",
        "                eval_strategy=\"epoch\",\n",
        "                per_device_eval_batch_size=size,\n",
        "                per_device_train_batch_size=size,\n",
        "                num_train_epochs=epochs,\n",
        "                learning_rate=rate,\n",
        "                disable_tqdm=True,\n",
        "                report_to=\"none\"\n",
        "            )\n",
        "            trainer = Trainer(\n",
        "                model,\n",
        "                training_args,\n",
        "                train_dataset=tokenized_datasets[\"train\"],\n",
        "                eval_dataset=tokenized_datasets[\"validation\"],\n",
        "                data_collator=data_collator,\n",
        "                processing_class=tokenizer,\n",
        "                compute_metrics=compute_metrics\n",
        "            )\n",
        "\n",
        "            # Train the model\n",
        "            result |= trainer.train().metrics\n",
        "\n",
        "            # Stop tracking emissions\n",
        "            tracker.stop()\n",
        "\n",
        "            # Store metrics and delete the tracker\n",
        "            result |= trainer.evaluate() | get_trainable_parameters(model) | tracker.final_emissions_data.values\n",
        "          finally:\n",
        "            del tracker\n",
        "\n",
        "          lora_results.append(result)\n",
        "\n",
        "  write_results_to_csv(output_dir + model_name +'-lora-results.csv', lora_results)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  runtime_seconds = end_time - start_time\n",
        "  runtime_hours = runtime_seconds / 3600\n",
        "\n",
        "  # Output the best result\n",
        "  max_lora_result = lora_results[0]\n",
        "  for _, result in enumerate(lora_results):\n",
        "    if result['eval_f1'] > max_lora_result['eval_f1']:\n",
        "      max_lora_result = result\n",
        "  print(f'\\n================ \\\n",
        "          \\nTotal Runtime: {runtime_hours} hours \\\n",
        "          \\nBest Result: \\\n",
        "          \\n\\tF1={max_lora_result[\"eval_f1\"]} \\\n",
        "          \\n\\tRank={max_lora_result[\"rank\"]} \\\n",
        "          \\n\\tAlpha={max_lora_result[\"alpha\"]} \\\n",
        "          \\n\\tBatch Size={max_lora_result[\"batch_size\"]} \\\n",
        "          \\n\\tLearning Rate={max_lora_result[\"learning_rate\"]}'\n",
        "  )\n",
        "\n",
        "  return lora_results"
      ],
      "metadata": {
        "id": "2sRCUocTD3eo",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Program"
      ],
      "metadata": {
        "id": "5BssM-XUfY3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"prajjwal1/bert-tiny\", \"prajjwal1/bert-mini\", \"prajjwal1/bert-small\"]"
      ],
      "metadata": {
        "id": "HkkG0wwLWcDD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters from [google-research/bert](https://github.com/google-research/bert):\n",
        "\n",
        "\n",
        "\n",
        "> For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs:\n",
        "> * batch sizes: 8, 16, 32, 64, 128\n",
        "> * learning rates: 3e-4, 1e-4, 5e-5, 3e-5\n",
        "\n"
      ],
      "metadata": {
        "id": "aZnBuTehsapX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4\n",
        "batch_sizes = [8]#, 16, 32, 64, 128]\n",
        "learning_rates = [3e-4]#, 1e-4, 5e-5, 3e-5]"
      ],
      "metadata": {
        "id": "62XZoxhusMhO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranks = [1]#, 2, 4, 8, 16]\n",
        "alphas = [1]#, 2, 4, 8, 16]"
      ],
      "metadata": {
        "id": "niceYTIvGnz2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models:\n",
        "  hyperparameter_tune(model_name, batch_sizes, learning_rates)\n",
        "  lora_hyperparameter_tune(model_name, ranks, alphas, epochs, batch_sizes, learning_rates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfif1Jzfdn4v",
        "outputId": "b81d6d6d-e3dd-4f44-c46b-76d4f44b9ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 03:40:36] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 03:40:36] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 03:40:36] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 03:40:36] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 03:40:36] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 03:40:37] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 03:40:37] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 03:40:37] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 03:40:37]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 03:40:37]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 03:40:37]   CodeCarbon version: 2.7.4\n",
            "[codecarbon INFO @ 03:40:37]   Available RAM : 50.994 GB\n",
            "[codecarbon INFO @ 03:40:37]   CPU count: 8\n",
            "[codecarbon INFO @ 03:40:37]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 03:40:37]   GPU count: 1\n",
            "[codecarbon INFO @ 03:40:37]   GPU model: 1 x Tesla T4\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[codecarbon INFO @ 03:40:52] Energy consumed for RAM : 0.000080 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 03:40:52] Energy consumed for all GPUs : 0.000066 kWh. Total GPU Power : 15.9386194703106 W\n",
            "[codecarbon INFO @ 03:40:52] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 03:40:52] 0.000323 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': nan, 'eval_accuracy': 0.3161764705882353, 'eval_f1': 0.0, 'eval_runtime': 1.2633, 'eval_samples_per_second': 322.963, 'eval_steps_per_second': 40.37, 'epoch': 1.0}\n",
            "{'loss': 117.5354, 'grad_norm': nan, 'learning_rate': 5.821350762527233, 'epoch': 1.0893246187363834}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPGKI1yRcV9NpsMklCsRUnk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}